---
layout: post
title: Blog Post 6
---

In this Blog Post, I will develop and assess a fake news classifier using Tensorflow.

We will work Google Colab. When training models, enabling a GPU runtime (under Runtime -> Change Runtime Type) is likely to lead to significant speed benefits. [Here's the code for this post.](https://colab.research.google.com/drive/1pE0tqnhAmdQpwqDQYsBcDHu84qBsSg2L?usp=sharing)

### Data Source
Our data for this assignment comes from the article

- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).

Prof.Chodrow accessed it from Kaggle. He has done a small amount of data cleaning for you already, and performed a train-test split.

## §1. Load Packages and Acquire Training Data

First let's load all the packages we're gonna use.
```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string
from matplotlib import pyplot as plt


from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

# requires update to tensorflow 2.4
# >>> conda activate PIC16B
# >>> pip install tensorflow==2.4
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Then let's load the training data set.
```python
df = pd.read_csv('https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true')
df.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

We can see that each pieace of data contains `title`, the title of the news, `text`, the body of the news, and `fake`. `fake` is 1 if it is a fake news and 0 otherwise.

## §2. Make a Dataset

We want to write a function called `make_dataset`. This function achieve two things:

1. Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.
2. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult these lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.

Before removing the stopwords, we need to import nltk module and download the stopwords.
```python
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
```

Now let's write the function, which takes in a dataframe and return a `tf.data.Dataset` we want. It's also ready for batching.
```python
def make_dataset(df):
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  dataset = tf.data.Dataset.from_tensor_slices(
        (
          {
              "title" : df[["title"]], 
              "text" : df[["text"]]
          }, 
          {
              "fake" : df[["fake"]]
          }
         )
        )
  dataset.batch(100)
  return dataset
```

### Validation Data

Let's construct the `Dataset` and split 20% of it to use for validation.
```python
data = make_dataset(df)

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size).batch(20)
val   = data.skip(train_size).take(val_size).batch(20)

len(train), len(val)
```

    (898, 225)

### Base Rate

Before we build any model. let's check on the base rate.

```python
df['fake'].value_counts()
```

    1    11740
    0    10709
    Name: fake, dtype: int64


```python
base_rate = 11740/(11740+10709)
base_rate
```
    0.522963160942581

The base rate is about 52.3% and our model should perform better than this.

## §3. Create Model

For the following models we build, we want to answer this question:
> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

Hence, we need to create three (3) TensorFlow models.

1. In the first model, you should use only the article title as an input.
2. In the second model, you should use only the article text as an input.
3. In the third model, you should use both the article title and the article text as input.

### Titile Model

To process natural languages, we need to vectorize them first. Let's create the vectorizer layer and construct the model. 

```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer_title = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer_title.adapt(train.map(lambda x, y: x["title"]))
```
```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)
```

```python
title_features = vectorize_layer_title(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_output = layers.Dense(2, name = "fake")(title_features)
```


```python
model1 = keras.Model(
    inputs = [title_input],
    outputs = title_output
)
```

Now let's compile the model.
```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model1.fit(train, 
                    validation_data=val,
                    epochs = 20, 
                    verbose = True)
```

```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```

![image-example.png]({{ site.baseurl }}/images/bp61.png)

The accuray is for our model1, title only model, is between 97% and 98%. The validation accuracy is about 96% to 97%. We may experience some degree of overfitting.

### Text Model

Different from the first model, our second model only takes text as input.

```python
vectorize_layer_text = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500)
vectorize_layer_text.adapt(train.map(lambda x, y: x["text"]))
```

```python
text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

```python
text_features = vectorize_layer_text(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding_text")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_output = layers.Dense(2, name = "fake")(text_features)
```

```python
model2 = keras.Model(
    inputs = [text_input],
    outputs = text_output
)
```

```python
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

```python
history = model2.fit(train, 
                    validation_data=val,
                    epochs = 20, 
                    verbose = True)
```

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.legend()
```
![image-example.png]({{ site.baseurl }}/images/bp62.png)

The accuray is for our model1, text only model, is between 96% and 98%. The validation accuracy is about 96% to 97%. The text model perform not as well as the title model. It may be that titles of fake news often use eyes-catching words.

# Title and Text Model

Our final model takes in both title and text.

```python
tt_features = layers.concatenate([text_features, title_features], axis = 1)
tt_output = layers.Dense(2, name = "fake")(tt_features)
```

```python
model3= keras.Model(
    inputs = [title_input, text_input],
    outputs = tt_output
)
```

```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

```python
history = model3.fit(train, 
                    validation_data=val,
                    epochs = 20, 
                    verbose = True)
```

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.legend()
```

![image-example.png]({{ site.baseurl }}/images/bp62.png)

The training accuracy is above 99.5% and the validation accuracy is between 99% and 99.5%. It's far beyond my expectation. Therefore, it proves that to it is most effective to focus on both titles and the article to detect fake news.

## §4. Model Evaluation

First, we import the validation data and transform it to tensorflow dataset.

```python
test = pd.read_csv('https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true')
```

```python
data_test = make_dataset(test)
data_test_size = int(len(data_test))

data_test_final = data_test.take(data_test_size).batch(20)
```

Let's evaluate our best model, the title and text model, with our validation data.

```python
model3.evaluate(data_test_final)
```

    1123/1123 [==============================] - 8s 7ms/step - loss: 0.0617 - accuracy: 0.9873


    [0.061730124056339264, 0.9873490929603577]


The accuracy is 98.73%, above than the goal of 97%.

## §5. Embedding Visualization

Let's inspect the embedding our model learned. We use the same method as introduced in the lecture.


```python
weights = model3.get_layer('embedding_text').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later
```


```python
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>0.013350</td>
      <td>-0.004910</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.079975</td>
      <td>-0.015826</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>2.020064</td>
      <td>0.025933</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>-0.748778</td>
      <td>-0.038196</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>1.277741</td>
      <td>-0.035450</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>setting</td>
      <td>0.131741</td>
      <td>0.046138</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>sector</td>
      <td>0.117145</td>
      <td>-0.104074</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>quoted</td>
      <td>2.250577</td>
      <td>-0.007476</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>hoped</td>
      <td>1.719677</td>
      <td>0.001193</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>hampshire</td>
      <td>0.219284</td>
      <td>-0.093732</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>




```python
import plotly.express as px 

fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```

![image-example.png]({{ site.baseurl }}/images/bp64.png)

With the embedding graph, we can find some associations in the words.

```python
group1 = ["protest", "security", "policy"]
group2 = ["problem", "investigators", "shot"]
group3 = ["population", "communities", "transgender"]

def mapper(x):
    if x in group1:
        return 1
    elif x in group2:
        return 2
    elif x in group3:
        return 3
    else:
        return 0

embedding_df["highlight"] = embedding_df["word"].apply(mapper)
embedding_df["size"]      = np.array(1.0 + 50*(embedding_df["highlight"] > 0))
```


```python
import plotly.express as px 

fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 color = "highlight",
                 size = list(embedding_df["size"]),
                 size_max = 10,
                 hover_name = "word")

fig.show()
```
![image-example.png]({{ site.baseurl }}/images/bp64.png)

As highlighted in the graph, social movement related words, including protest, security, and policy appear together. Words that are oftern related to crimes also have strong associations, including problem, investigators, shot. Meanwhile, words like population, communites, transgender also have strong relationship.
