---
layout: post
title: Blog Post 4
---

In this blog post, I'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. Each of the below parts will pose to you one or more specific tasks.

As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, I will derive and implement spectral clustering. 

## Preparation

We first import all the modules we need.

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

And here's the make_moons datasets we generate to be clustered.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
```

## Part A

In this part, we construct the *similarity matrix* $$\mathbf{A}$$ of `(n, n)` size. Entry `A[i,j]` is set to be `1` if the data point `i`) is within distance `epsilon` of the data point `j`, and `0` otherwise. 

```python
#use pairwise_distances function from sklearn to take the distance
#the function takes in a vector and return a distance matrix
from sklearn.metrics.pairwise import pairwise_distances
A = pairwise_distances(X)

epsilon = 0.4

# np.where(conditions, it true, if false)
A = np.where(A <= epsilon, 1, 0)
# fill the diagonal with 0
np.fill_diagonal(A, 0)
```

Now we can check whether $$\mathbf{A}$$ is the similarity matrix we want.

```python
A
```

    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])

Everything seems good!


## Part B

We need to classify data points either $$C_0$$ or $$C_1$$ claster. The label result will be an array `y`. If `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.  

To evaluate our clusters, we introduce the *binary norm cut objective*. A pair of good clusters will have a relatively small norm cut. The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $i$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

Let's compute the *binary norm cut objective*.

#### B.1 The Cut Term

The `cut(A,y)` function takes in the data points and the label and then computes the cut term.

```python
def cut(A,y):
    cut = 0
    for i in range(len(y)):
        for j in range(len(y)):
            if y[i]!=y[j]:
                cut = cut + A[i,j]
    return cut
```

Let's apply the function to our dataset.

```python
cut(A,y)
```
    26

We obtain a result of 26. Let's then see the cut term of a random vector.

```python
# generate a random vector
r = np.random.randint(2, size = n)
cut(A,r)
```
    2184

The cut term is much more smaller for our label.

#### B.2 The Volume Term 

The function `vols(A,y)` computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple.

```python
#PartB.2
def vols(A, y):
    
    v0 = A[y==0,:].sum()
    v1 = A[y==1,:].sum() 
    
    return v0,v1
```

Let's apply it to our dataset.

```python
vols(A, y)
```

    (2299, 2217)

Now we are ready to compute the normcut.

```python
def normcut(A,y):
    v0,v1 = vols(A,y)
    normcut = cut(A,y) * (1/v0 + 1/v1)
    return normcut
```

If we compare the results of the label vector and a random vector, we will find that the label vector y yields a much smaller normcut.

```python
normcut(A,y), normcut(A,r)
```

    (0.02303682466323045, 1.9536818485425447)

## Part C

to find a cluster vector `y` we can try to minimize `normcut(A,y)`. Here we introduce a new vector $$\mathbf{z}$$ defined \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

We can calculate $\mathbf{z} using the folloing code:

```python
def transform(A, y):
    v0,v1 = vols(A,y)
    z = np.where(y==0,  1/v0 , - 1/v1 )
    return z
```

Then we transform our normcut equation using $$\mathbf{z}$$. The equation would 

be $$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

$$\mathbf{D}$$ is a matrix with nonzero entries row-sum on the diagonal.

To prove the equation, we compute the lefthand-side of the equation as below and find that the result is the same as the normcut we've obtained in part B.

```python
# Prove the equation
# left hand side
D = np.full_like(A, 0)
np.fill_diagonal(D, np.sum(A, axis=1))
z = transform(A, y)
lefthand_side = 2*(z@(D-A)@z)/(z@D@z)
lefthand_side
```
    0.02303682466323018

Now we check $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones. This equation tells us $$\mathbf{z}$$ has the same amount of positive and negative entries.

```python
np.isclose (z.T@D@np.ones(n) , 0)
```
    True

## Part D

We can minimize the normcut by minimzing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$.

We use the orth_obj funtion below to bake the condition:

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Now we can use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$ and obtain a minimizing vector `z_min`. 

```python
import scipy
z_ = scipy.optimize.minimize(orth_obj, z)
z_
```

## Part E

By design, `z_min[i]` actually contains information about the cluster label of data point `i`. We can now try to plot the data points and see whether we get the correct clusters.

```python
z_min = z_.x
plt.scatter(X[:,0], X[:,1], c = z_min < 0)
```

![image-example.png]({{ site.baseurl }}/images/bp41.png)

It looks like z_min fails. If we check back on the `scipy.optimize.minimize` function, it seems that the success status is false.

<div class="got-help">
I asked a friend who had taken PIC16B before about the problem. She said there is something wrong with the minimize function, but it would work if we change the threshold to -0.0015.
</div>
{::options parse_block_html="false" /}

Now let's try again.

```python
plt.scatter(X[:,0], X[:,1], c = z_min < -0.0015)
```

![image-example.png]({{ site.baseurl }}/images/bp42.png)

This time we get a pretty good clustered plot.

## Part F

The optimizing method is too slow to be practical and it sometimes goes run as in Part E. We introduce a mathematical trick here.

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

The vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue.

To obtain the desired eigenvector $$\mathbf{z}$$, we
1. Construct the Laplacian matrix. 
2. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.

```python
# Part F
# calculate the Laplacian matrix 
L = np.linalg.inv(D)@(D-A) 
# find all eigenvalues
w, v = np.linalg.eig(L)
# sort eigenvalue
idx = w.argsort()
w = w[idx]
v = v[:,idx]
#get the second smallest eigenvector
z_eig = v[:, 1]
```

How did we do?

```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```
![image-example.png]({{ site.baseurl }}/images/bp43.png)

Well-done! Only one point is misclassified.

## Part G

Now we synthesize our results from the previous parts. We write a function `spectral_clustering(X, epsilon)` which takes in the input data `X` and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`.

Given data, the function: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def spectral_clustering(X, epsilon):
    '''
    A function that takes in the data points and the distance threshold and returns a binary labeling.
    
    X: the input data, a nxn matrix containing the Euclidean coordinates of the data points
    epsilon: the distance threshold to construct the similarity matrix, two points within it would be considered as in the same cluster
    
    yield: an array of binary labels indicating whether data point i is in group 0 or group 1
    '''
    # Construct the similarity matrix
    A = pairwise_distances(X)
    A = np.where(A <= epsilon, 1, 0)
    np.fill_diagonal(A, 0)
    
    # Construct laplacian matrix
    D = np.full_like(A, 0)
    np.fill_diagonal(D, np.sum(A, axis=1))
    L = np.linalg.inv(D)@(D-A) 
    
    # Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    w, v = np.linalg.eig(L)
    
    # Extract the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    v = v[:,w.argsort()]
    
    return np.where(v[:, 1]<0, 1 ,0)
```

Apply the function to our previous data, we are able to get the clustered plot.
```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![image-example.png]({{ site.baseurl }}/images/bp43.png)

## Part H

In this part we inspect on how the 'noise' influence our cluster results. We increase noise for each graph. Notice that we change our sample size to 1000 since our powerful function is able to handle a larger dataset.

```python
# Graph 1
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```
![image-example.png]({{ site.baseurl }}/images/bp44.png)

As we increase noitse to 0.1, we could still get a good spectral clustering.

```python
# Graph 2
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```
![image-example.png]({{ site.baseurl }}/images/bp45.png)

As the noise increases to 0.15, the boundary becomes a little bit unstable.

```python
# Graph 3
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

![image-example.png]({{ site.baseurl }}/images/bp46.png)

As the noise reaches 0.2, the boundary line is exactly linear and we cannot get a good clustering result.

Therefore, our powerful clustering function is still under the influence of noise. If the noise of the dataset is too large, we may not obtain a desiring clustering result.

## Part I

Now try your spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![image-example.png]({{ site.baseurl }}/images/bp47.png)

According to the hint, we try values of `epsilon` between `0` and `1.0`.


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```

![image-example.png]({{ site.baseurl }}/images/bp48.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```

![image-example.png]({{ site.baseurl }}/images/bp49.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.55))
```
![image-example.png]({{ site.baseurl }}/images/bp410.png)

From the graphs we see that a `epsilon` between '0.2' and '0.5' could give us a correct clustering. It maybe because the difference between the radius of the two circles is around 0.5.

